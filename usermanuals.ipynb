{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\rohit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from getpass import getpass\n",
    "import re\n",
    "nltk.download('punkt')   \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer, WhitespaceTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import math\n",
    "import pprint\n",
    "import pyttsx3\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "openai.api_key = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "User Manual  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 2 \n",
      "Table of Contents  \n",
      "Introduction ................................ ................................ ................................ ................................ ..............  3 \n",
      "High Level Process Flow  ................................ ................................ ................................ .........................  3 \n",
      "Login Information  ................................ ................................ ................................ ................................ .... 3 \n",
      "Workflow ................................ ................................ ................................ ................................ ..................  4 \n",
      "Reconciliation  ................................ ................................ ................................ ................................ ........  16 \n",
      "Other  Screens  ................................ ................................ ................................ ................................ ....... 16 \n",
      "Abbreviations  ................................ ................................ ................................ ................................ ........  21 \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 3 \n",
      "Introduction  \n",
      "It is a data conversion tool built on Oracle technologies, designed to automate data conversion from any \n",
      "source to Oracle Cloud Applications. With ConvertRite, you can automate manual, time -consuming and \n",
      "error -prone processes (such as data mapping and validation) and convert all your data from legacy \n",
      "applications to make it compatible with Oracle Cloud Applications.  \n",
      "High Level Process Flow \n",
      " \n",
      "Login Information  \n",
      "Link: https://convertrite.ritesoftware.com/  \n",
      "Login: The user should log in and select the role of admin to access ConvertRite.    \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 4 \n",
      "Workflow  \n",
      "Click on hamburger menu  on top left \n",
      "for application navigation.  \n",
      "       \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "➢ Project, POD, Parent object and child object  (object code)  are \n",
      "Master data.  \n",
      "STEP 1:  \n",
      "Creation  of POD:  \n",
      "POD - The Product Oriented Delivery (POD) model is  a software development strategy that centers on \n",
      "building small cross -functional teams that own specific tasks or requirements for a project.  \n",
      "A project can be created in different PODs or many Projects in one POD can be \n",
      "created.  \n",
      "• To create  a POD in ConvertRite, click on hamburger button - master data - \n",
      "lookup workbench - search for POD – click on add at lookup values  – add POD \n",
      "name.  (POD  is independent - doesn’t rel y on any other values) – save. \n",
      " \n",
      "➢ Export CSV – Export CSV button downloads all the file based data on this \n",
      "particular screen.  \n",
      " \n",
      " \n",
      " \n",
      "Creation of Project : \n",
      "• To create the project, in the lookup header – search for  the Lookup name as Project and add the Lookup \n",
      "value – your project details.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 5 \n",
      "• Project is related to POD – Hence select the required POD  – Save . \n",
      " \n",
      "Creation of Parent Object:  \n",
      "• Click on the hamburger button - Master data - Lookup workbench .  \n",
      "• Click on Search  -Search for Parent object - add a lookup value - define lookup name as required parent \n",
      "object from FBDI file ( Create parent obj ect) – Assign project name in actual value . \n",
      " \n",
      "• If the project is not listed under actual value, need to add project details under project name lookup name.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 6 \n",
      "Creation of Child Object:  \n",
      "• Search for lookup name – object  code  – add the lookup value for child object from FBDI file  which should \n",
      "be related to  the parent object code.  \n",
      " \n",
      " \n",
      "Step 2: \n",
      "Assign POD  and Project : \n",
      "• Click on the Hamburger button –ConvertRite  – Manage your project – New. \n",
      "• Define project name, POD and all mandatory information - save  (POD environment will be assigned to  the \n",
      "project and object in this screen) . \n",
      "         \n",
      "➢ Copy project is used to copy the complete project with same data into a different POD with project \n",
      "status, start and completion date.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 7 \n",
      " \n",
      "• Click on Load WBS – Project will be assigned to parent object.  \n",
      " \n",
      "• If the project is created already, search for the required project and Load WBS [it can be done for the \n",
      "other parent object (Load WBS is parent object level) ] \n",
      "Step 3:  \n",
      "Assign Role Object:  \n",
      "• We must  Enable flag  to activate the  load metadata option.   \n",
      "• Click on the Admin – switch to SuperUser. \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "• In the SuperUser profile, click on the hamburger button on top left - admin – role object.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 8 \n",
      " \n",
      "• Select the role name as admin, required POD, Project,  parent object – Enable flag. Once done, switch \n",
      "back to admin.  \n",
      " \n",
      "✓ Master data configuration done . \n",
      "• We must configure and connect cloud/source to database to get metadata so that stagging table can be \n",
      "created.  \n",
      "• On cloud side after completing configuration – load metadata should be done, on source  side – both can \n",
      "be done on same screen.  \n",
      "Step 4:  \n",
      "Source  Configuration:  (Parent object level)  \n",
      "• Click on the Hamburger button - Configuration – External Source Configuration – New – fill up info rmation  \n",
      "– Save ( Parent object level) – Upload file (View file provided by functional team).  \n",
      " \n",
      "• If parent object is available, we can use update view option to update any details . (To update view file for \n",
      "each individual child object) . \n",
      "• Click on the load metadata – structure is done . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 9 \n",
      "Step 5:  \n",
      "Source Template workbench:  \n",
      "• Click on the Hamburger button -ConvertRite - Source Template Workbench - New – fill all required fields – \n",
      "Save- Create Table – Import Columns.      \n",
      " \n",
      "➢ BU Specific must  be enabled assign the template at business user level.  \n",
      "➢ Check the Normalize box to differentiate duplicate data (Process will be stopped if any duplicate data \n",
      "is identified) . \n",
      "• Select all columns displayed - Save. \n",
      "• Click on the Baseline – Stagging table name and view name will be displayed if we click on three dots \n",
      "(Stagging table will have all the data but view have only required data) . \n",
      "• Select Ori g trans ref - To link source and cloud columns data , unique identification for the records  and \n",
      "also when  we n eed 2 or more columns data to merge the data to 1 column in cloud . \n",
      "➢ Re Orig  Tras ref is used when an Orig trans ref is already created and requirement is \n",
      "changed/updated - we can update re Orig trans based on the batch name . \n",
      "• Load Data – success (make a note  of the batch name – unique name should be given by us) . \n",
      " \n",
      "➢ While loading data, choose manual  to upload small data through file and choose external option to \n",
      "upload large data.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 10 \n",
      " \n",
      "Step 6: \n",
      "Cloud Access Configuration:  \n",
      "• Click on  the Hamburger button – Configuration  – Cloud Access configuration.  \n",
      "• Cloud Access Configuration screen is used to connect to  the cloud SaaS environment with the required \n",
      "credentials.  \n",
      "• Cloud URL – SaaS URL and credentials will be provided by the functional team.  \n",
      " \n",
      "Step 7:  \n",
      "Cloud Configuration:  \n",
      "• Click on the Hamburger button – Configuration – Cloud Configuration . \n",
      "• Click on New –Define all re quired  info (From FBDI ): Object Code – Ctrl File Name – Xlsm F ile Name -\n",
      "Sheet Name- Interface Table. \n",
      " \n",
      "• Hence Connected to cloud. We must get the structure – through load meta data step. ( Only after  role \n",
      "object -parent object level) . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 11 \n",
      "Step 8:  \n",
      "Cloud Load Metadata:  \n",
      "• Click on the Hamburger button -Master Data- Load metadata - select Cloud – Cloud Load Metadata . \n",
      " \n",
      "• On the pop - up screen, select all the required data  – Metadata table will be created.  \n",
      " \n",
      "➢ We can load metadata to source by selecting Source radio - click on EBS adapter.  \n",
      "➢ By Clicking on EBS adapter, it will re -direct to external source co nfiguration screen  where we can \n",
      "select particular parent object to create metadata , click on upload file and load data.  \n",
      "Step 9: \n",
      "Cloud Stagging table:  \n",
      "• Click on  the Hamburger button  - ConvertRite  – Cloud Template Workbench – New – fill all required \n",
      "information - Save – Create Table – Import Columns.  \n",
      " \n",
      "• In the source template field- the new template we have  created should be available,  select that source \n",
      "template  and SAVE .  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 12 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "• Source columns will be added and listed on this screen . \n",
      "• On the Cloud Template Workbench – click on the anchor icon ( on top right) which is called user hooks \n",
      "(Extraction, validation & transformation, and cloud import) . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "• We can enable pre or post hook to manipulate any data before or afte r validation . \n",
      "• Hook value = Any SQL query ( As provided) . \n",
      "Step 10:  \n",
      "Sequence Generator:  \n",
      "• Now to generate sequence – go to hamburger button - Sequence Generator/ Grouping – FBDI Workbench \n",
      "(Ctrl) – New - Fill all the details  – choose Auto  – Save. \n",
      "• Note: We can generate sequence using Ctrl file [FBDI workbench (ctrl)]   or  Xlsm file [FBDI workbench \n",
      "(xlsm)] \n",
      " \n",
      "• Switch back to Cloud Template Workbench – Seq+ ctrl button will be enabled, click on it and sort \n",
      "sequence . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 13 \n",
      " \n",
      "Step 11:   \n",
      "• We must do the mapping (after generating sequence) – information will be provided by source/functional \n",
      "team.  \n",
      "Mapping types:   \n",
      "1. As – Is - Moves X - X \n",
      "2. Prefix  - Add X - …... (Before data value)  \n",
      "3. Suffix  - Add …... - X (After the data value)  \n",
      "4. Constant  - Source no v alue but should sent constant value to cloud  \n",
      "5. One to One  - One field of source will be mapped to one cloud value  \n",
      "6. Two to One  - Two fields of source will be mapped to one cloud value  \n",
      "7. Three to One  - Three fields of source will be mapped to one cloud value  \n",
      "8. Formula set - Write query to do other conversions (optional values)  \n",
      "• Click on the hamburger button - Data Transformation - Define Mapping Set (To Define  1-1, 2-1, 3-1) -New- \n",
      "define all information and save.  \n",
      "• Once the mapping set is saved, add mapping set value s (column – column) . \n",
      " \n",
      "• Source Object  - View file  from the source template  \n",
      "• Source Column – Column name in the source side  \n",
      "• Source Field – Column value in the source side  \n",
      "• Cloud Column – Column name from the cloud s ide (This  information will be provided by functional  team ) \n",
      "• Cloud Value –Column  value to be updated on the cloud side \n",
      "• We must  write the condition for 1 -1, 2-1 and 3 -1 mappings in W here Clause . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 14 \n",
      "• SQL query will be generated by the application  on the backend  and hence transforms the data . \n",
      "Define Mapping Set: \n",
      "• To define any other mapping type to transform the data  - Click on menu - Data Transformation - Define \n",
      "Formula Set –we can write required SQL query.  \n",
      "• We have 3 Formula types – Java, SQL and function . \n",
      "• Test SQL is used to test the SQL query written by us.  \n",
      "• Test Data  is used to test the data with given SQL query at Orig trans level.  \n",
      "• Original Trans Ref – The records that we select as org trans ref in  the source side . \n",
      "• Formula value – Expected value should be given. \n",
      "• SQL Query – Should be written on Org trans ref . \n",
      "• Test SQL – To test the SQL query written . \n",
      "• Test Data – To test the data that we got by the sql query written.  \n",
      " \n",
      "Step 1 2: \n",
      "In Cloud template workbench:  \n",
      "• Select all the columns for which sequence is generated – Save - Click on Baseline to create table.  \n",
      " \n",
      "Step 1 3:      \n",
      "Validation : \n",
      "• Click on the Hamburger  button - ConvertRite - Cloud Conversions Workbench – select  the template name \n",
      "and batch name [get that from source template workbench(unique) ] \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 15 \n",
      " \n",
      "• Click on the transform to validate and convert.  \n",
      "• Once done , ConvertRite  application needs to be connected to oracle fusion  to run the jobs.   \n",
      "➢ Re-Process option is used to re -validate only failed records after correcting the errors.  \n",
      "➢ FBDI – File based data import - is the file generated after the conver sions in our application before \n",
      "moving into the cloud.  \n",
      "➢ FBDI – is only for ERP modelling, HDL – is for HCM related data.  \n",
      " \n",
      "Step 1 4:    \n",
      "Load Import Metadata:  \n",
      "• So, click on the hamburger button -Go to the Master  Data option under  – Load Import Metadata – fill the \n",
      "information provided by  the respective  functional team – Save. \n",
      " \n",
      "Step 1 5:      \n",
      "Cloud Load Import:  \n",
      "• Click on the hamburger button - ConvertRite - Cloud Load Import – select  template name , batch name, \n",
      "parameter list –based on the parent object  level.  \n",
      " \n",
      "• Click on the Load Import – request will be submitted,  and result ID should be generated.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 16 \n",
      "➢ Recon cile – When we click this button, it hits the template and compares.  \n",
      "➢ Reconcile Report – Generates all the reports Pass/Fail . \n",
      "➢ Rejection Report  – Only the details of rejected results in reconcile.  \n",
      "Reconciliation  \n",
      "Data reconciliation (DR) is a term typically used to describe  a verific ation phase during a data migration \n",
      "where the target data is compared against original source data to ensure that the migration architecture has \n",
      "transferred the data correctly.  Reconciliation is the process of ensuring that two sets of records are in \n",
      "agree ment.   \n",
      "• Click on the hamburger button - Dash Boards – Reconcile – select object,  cloud template,  cloud  batch - \n",
      "view.  \n",
      " \n",
      "• We will get the results of source data extracted, validated and data migrated to the cloud . \n",
      "• We can check error records and details.  \n",
      "Other  Screens  \n",
      "Load Cockpit:  \n",
      "Let’s say object and the flow is already created – to load the data - transform and cloud data loading for the \n",
      "same parent object with different child objects in the same screen.  \n",
      "• We can use Load Cockpit option from the Hamburger button - Automation - Load Cockpit – New – fill up all \n",
      "the information.  \n",
      "• Cloud and Source templates should be created/available to use Load Cockpit option.  \n",
      " \n",
      "• Click on Load Source Data button – same functionality as to Load Data from Source Template \n",
      "Workbench.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 17 \n",
      "• Enter the U nique Batch Name.  \n",
      "• Click on the T ransform to validate all mapping of Cloud Template Workbench screen.  \n",
      " \n",
      "• If you want to upload file manually, choose manual radio button . \n",
      "• If the  file-based data is different for all the object s– then upload the file from Load Source ( Denormalize ) \n",
      "and if file based data is same for all the objects then File can be uploaded from load source (Normalise) . \n",
      "Source Conversions workbench:  \n",
      "• Click on the Hamburger button - ConvertRite – Source Conversions Workbench - select the template to \n",
      "check  failed records.  \n",
      " \n",
      "• In this screen we can check  and access  failed records when we are extracting/loading source data . \n",
      "• Download option under status will be enabled only when we use external option to upload file in the \n",
      "source template workb ench . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 18 \n",
      "HCM Load Import:  \n",
      "• Click on the hamburger button  - ConvertRite – HCM Load import – New –fill all required details  - Load \n",
      "and Import.  \n",
      " \n",
      "• HCM Load Import screen is used to migrate data directly into cloud when we are dealing with HCM object \n",
      "codes.  \n",
      "• Summary Button hits the summary of the cloud table and Summary report generates the file with \n",
      "summary data.  \n",
      "Date Configuration:  \n",
      "• Click on the hamburger button – Configuration – Date Configuration  - New- Fill details - Save. \n",
      " \n",
      "• Date configuration screen is used to change the source date format before converting the data as the \n",
      "required date format in cloud.  \n",
      "FBDI Workbench (XLSM):  \n",
      "• Click on the hamburger button – Sequence Generator/Grouping – FBDI Workbench (X lsm) -New – Save.  \n",
      " \n",
      "• FBDI Workbench ( Xlsm) screen is used to generate sequence based on the xlsm file in  the cloud \n",
      "database.  \n",
      "➢ Download comments – downloads the column details with description.  \n",
      "➢ Save as Blob – History of object sequence with version will be saved for future reference.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 19 \n",
      "Object Grouping:  \n",
      "• Click on the hamburger button - Sequence Generator/Grouping – Object Grouping . \n",
      "• Object grouping is used to group the objects based on Excel File of Cloud.  \n",
      " \n",
      "• If we do Objec t grouping, template grouping should be done.  \n",
      "Template Grouping:  \n",
      "• Click on  the hamburger button - Sequence Generator/Grouping –Template Grouping . \n",
      " \n",
      "• Template Grouping screen is used to group the same objects that are grouped in  the object grouping by \n",
      "selecting template.  \n",
      "Cloud Sql Adhoc Query:  \n",
      "• Click on the hamburger button – Master data – Cloud SQL Adhoc Query  –New – Write SQL query .  \n",
      " \n",
      "• This screen is used to get the lookups from cloud with SQL query.  \n",
      "• Check the Update cloud lookup data box to update data in the lookup.  \n",
      "Load Cockpit – HCM:  \n",
      "• Click on the hamburger button - Automation  – Load Cockpit HCM  – fill the required info – Load Source \n",
      "Data (Auto) – Transform – HCM Cloud Import . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 20 \n",
      " \n",
      "• This screen is used to do the process in on e screen for HCM object codes.  \n",
      "Status of Jobs:  \n",
      "• Click on  the hamburger button -Dash Boards – Status of jobs . \n",
      " \n",
      " Help:  \n",
      "• Click on  the hamburger button – Dash Boards – Help. \n",
      ".  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 21 \n",
      "Abbreviations  \n",
      "Abbreviation  Full Form  \n",
      "ERP Enterprise Resource Planning  \n",
      "POD  Product Oriented Delivery  \n",
      "HDL HCM Data Loader  \n",
      "FBDI  File Based Data Import  \n",
      "Orig Trans Ref Original Transaction References  \n",
      "BU Business Unit  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "pdf_file = open('Revised ConvertRite User Manual - New_V3.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "num_pages = len(pdf_reader.pages)\n",
    "pdf_text=\"\"\n",
    "wordcount=0\n",
    "for page in range(num_pages):\n",
    "    pdf_page = pdf_reader.pages[page]\n",
    "    pdf_text += pdf_page.extract_text()\n",
    "print(pdf_text)\n",
    "pdf_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '', '', ' \\nUser Manual ', '', '', '', ' - User Manual  | Confidential  Page | 2 \\nTable of Contents  \\nIntroduction ................................ ................................ ................................ ................................ ..............  3 \\nHigh Level Process Flow  ................................ ................................ ................................ .........................  3 \\nLogin Information  ................................ ................................ ................................ ................................ .... 3 \\nWorkflow ................................ ................................ ................................ ................................ ..................  4 \\nReconciliation  ................................ ................................ ................................ ................................ ........  16 \\nOther  Screens  ................................ ................................ ................................ ................................ ....... 16 \\nAbbreviations  ................................ ................................ ................................ ................................ ........  21 \\n ', '', ' - User Manual  | Confidential  Page | 3 \\nIntroduction  \\nIt is a data conversion tool built on Oracle technologies, designed to automate data conversion from any \\nsource to Oracle Cloud Applications. With ConvertRite, you can automate manual, time -consuming and \\nerror -prone processes (such as data mapping and validation) and convert all your data from legacy \\napplications to make it compatible with Oracle Cloud Applications.  \\nHigh Level Process Flow', 'Login Information  \\nLink: https://convertrite.ritesoftware.com/  \\nLogin: The user should log in and select the role of admin to access ConvertRite.   ', '', ' \\n - User Manual  | Confidential  Page | 4 \\nWorkflow  \\nClick on hamburger menu  on top left \\nfor application navigation.  \\n      ', '', ' \\n➢ Project, POD, Parent object and child object  (object code)  are \\nMaster data.  \\nSTEP 1:  \\nCreation  of POD:  \\nPOD - The Product Oriented Delivery (POD) model is  a software development strategy that centers on \\nbuilding small cross -functional teams that own specific tasks or requirements for a project.  \\nA project can be created in different PODs or many Projects in one POD can be \\ncreated.  \\n• To create  a POD in ConvertRite, click on hamburger button - master data - \\nlookup workbench - search for POD – click on add at lookup values  – add POD \\nname.  (POD  is independent - doesn’t rel y on any other values) – save.', '➢ Export CSV – Export CSV button downloads all the file based data on this \\nparticular screen. ', '', 'Creation of Project : \\n• To create the project, in the lookup header – search for  the Lookup name as Project and add the Lookup \\nvalue – your project details. ', '', ' \\n - User Manual  | Confidential  Page | 5 \\n• Project is related to POD – Hence select the required POD  – Save .', 'Creation of Parent Object:  \\n• Click on the hamburger button - Master data - Lookup workbench .  \\n• Click on Search  -Search for Parent object - add a lookup value - define lookup name as required parent \\nobject from FBDI file ( Create parent obj ect) – Assign project name in actual value .', '• If the project is not listed under actual value, need to add project details under project name lookup name. ', '', '', ' \\n - User Manual  | Confidential  Page | 6 \\nCreation of Child Object:  \\n• Search for lookup name – object  code  – add the lookup value for child object from FBDI file  which should \\nbe related to  the parent object code. ', ' \\nStep 2: \\nAssign POD  and Project : \\n• Click on the Hamburger button –ConvertRite  – Manage your project – New. \\n• Define project name, POD and all mandatory information - save  (POD environment will be assigned to  the \\nproject and object in this screen) . \\n         \\n➢ Copy project is used to copy the complete project with same data into a different POD with project \\nstatus, start and completion date. ', '', ' \\n - User Manual  | Confidential  Page | 7', '• Click on Load WBS – Project will be assigned to parent object. ', '• If the project is created already, search for the required project and Load WBS [it can be done for the \\nother parent object (Load WBS is parent object level) ] \\nStep 3:  \\nAssign Role Object:  \\n• We must  Enable flag  to activate the  load metadata option.   \\n• Click on the Admin – switch to SuperUser.', '', ' \\n• In the SuperUser profile, click on the hamburger button on top left - admin – role object. ', '', '', ' - User Manual  | Confidential  Page | 8', '• Select the role name as admin, required POD, Project,  parent object – Enable flag. Once done, switch \\nback to admin. ', '✓ Master data configuration done . \\n• We must configure and connect cloud/source to database to get metadata so that stagging table can be \\ncreated.  \\n• On cloud side after completing configuration – load metadata should be done, on source  side – both can \\nbe done on same screen.  \\nStep 4:  \\nSource  Configuration:  (Parent object level)  \\n• Click on the Hamburger button - Configuration – External Source Configuration – New – fill up info rmation  \\n– Save ( Parent object level) – Upload file (View file provided by functional team). ', '• If parent object is available, we can use update view option to update any details . (To update view file for \\neach individual child object) . \\n• Click on the load metadata – structure is done .', '', ' \\n - User Manual  | Confidential  Page | 9 \\nStep 5:  \\nSource Template workbench:  \\n• Click on the Hamburger button -ConvertRite - Source Template Workbench - New – fill all required fields – \\nSave- Create Table – Import Columns.     ', '➢ BU Specific must  be enabled assign the template at business user level.  \\n➢ Check the Normalize box to differentiate duplicate data (Process will be stopped if any duplicate data \\nis identified) . \\n• Select all columns displayed - Save. \\n• Click on the Baseline – Stagging table name and view name will be displayed if we click on three dots \\n(Stagging table will have all the data but view have only required data) . \\n• Select Ori g trans ref - To link source and cloud columns data , unique identification for the records  and \\nalso when  we n eed 2 or more columns data to merge the data to 1 column in cloud . \\n➢ Re Orig  Tras ref is used when an Orig trans ref is already created and requirement is \\nchanged/updated - we can update re Orig trans based on the batch name . \\n• Load Data – success (make a note  of the batch name – unique name should be given by us) .', '➢ While loading data, choose manual  to upload small data through file and choose external option to \\nupload large data. ', '', ' \\n - User Manual  | Confidential  Page | 10', 'Step 6: \\nCloud Access Configuration:  \\n• Click on  the Hamburger button – Configuration  – Cloud Access configuration.  \\n• Cloud Access Configuration screen is used to connect to  the cloud SaaS environment with the required \\ncredentials.  \\n• Cloud URL – SaaS URL and credentials will be provided by the functional team. ', 'Step 7:  \\nCloud Configuration:  \\n• Click on the Hamburger button – Configuration – Cloud Configuration . \\n• Click on New –Define all re quired  info (From FBDI ): Object Code – Ctrl File Name – Xlsm F ile Name -\\nSheet Name- Interface Table.', '• Hence Connected to cloud. We must get the structure – through load meta data step. ( Only after  role \\nobject -parent object level) .', '', ' \\n - User Manual  | Confidential  Page | 11 \\nStep 8:  \\nCloud Load Metadata:  \\n• Click on the Hamburger button -Master Data- Load metadata - select Cloud – Cloud Load Metadata .', '• On the pop - up screen, select all the required data  – Metadata table will be created. ', '➢ We can load metadata to source by selecting Source radio - click on EBS adapter.  \\n➢ By Clicking on EBS adapter, it will re -direct to external source co nfiguration screen  where we can \\nselect particular parent object to create metadata , click on upload file and load data.  \\nStep 9: \\nCloud Stagging table:  \\n• Click on  the Hamburger button  - ConvertRite  – Cloud Template Workbench – New – fill all required \\ninformation - Save – Create Table – Import Columns. ', '• In the source template field- the new template we have  created should be available,  select that source \\ntemplate  and SAVE . ', '', '', ' - User Manual  | Confidential  Page | 12', '', '', '', ' \\n• Source columns will be added and listed on this screen . \\n• On the Cloud Template Workbench – click on the anchor icon ( on top right) which is called user hooks \\n(Extraction, validation & transformation, and cloud import) .', '', '', '', ' \\n• We can enable pre or post hook to manipulate any data before or afte r validation . \\n• Hook value = Any SQL query ( As provided) . \\nStep 10:  \\nSequence Generator:  \\n• Now to generate sequence – go to hamburger button - Sequence Generator/ Grouping – FBDI Workbench \\n(Ctrl) – New - Fill all the details  – choose Auto  – Save. \\n• Note: We can generate sequence using Ctrl file [FBDI workbench (ctrl)]   or  Xlsm file [FBDI workbench \\n(xlsm)]', '• Switch back to Cloud Template Workbench – Seq+ ctrl button will be enabled, click on it and sort \\nsequence .', '', ' \\n - User Manual  | Confidential  Page | 13', 'Step 11:   \\n• We must do the mapping (after generating sequence) – information will be provided by source/functional \\nteam.  \\nMapping types:   \\n1. As – Is - Moves X - X \\n2. Prefix  - Add X - …... (Before data value)  \\n3. Suffix  - Add …... - X (After the data value)  \\n4. Constant  - Source no v alue but should sent constant value to cloud  \\n5. One to One  - One field of source will be mapped to one cloud value  \\n6. Two to One  - Two fields of source will be mapped to one cloud value  \\n7. Three to One  - Three fields of source will be mapped to one cloud value  \\n8. Formula set - Write query to do other conversions (optional values)  \\n• Click on the hamburger button - Data Transformation - Define Mapping Set (To Define  1-1, 2-1, 3-1) -New- \\ndefine all information and save.  \\n• Once the mapping set is saved, add mapping set value s (column – column) .', '• Source Object  - View file  from the source template  \\n• Source Column – Column name in the source side  \\n• Source Field – Column value in the source side  \\n• Cloud Column – Column name from the cloud s ide (This  information will be provided by functional  team ) \\n• Cloud Value –Column  value to be updated on the cloud side \\n• We must  write the condition for 1 -1, 2-1 and 3 -1 mappings in W here Clause .', '', ' \\n - User Manual  | Confidential  Page | 14 \\n• SQL query will be generated by the application  on the backend  and hence transforms the data . \\nDefine Mapping Set: \\n• To define any other mapping type to transform the data  - Click on menu - Data Transformation - Define \\nFormula Set –we can write required SQL query.  \\n• We have 3 Formula types – Java, SQL and function . \\n• Test SQL is used to test the SQL query written by us.  \\n• Test Data  is used to test the data with given SQL query at Orig trans level.  \\n• Original Trans Ref – The records that we select as org trans ref in  the source side . \\n• Formula value – Expected value should be given. \\n• SQL Query – Should be written on Org trans ref . \\n• Test SQL – To test the SQL query written . \\n• Test Data – To test the data that we got by the sql query written. ', 'Step 1 2: \\nIn Cloud template workbench:  \\n• Select all the columns for which sequence is generated – Save - Click on Baseline to create table. ', 'Step 1 3:      \\nValidation : \\n• Click on the Hamburger  button - ConvertRite - Cloud Conversions Workbench – select  the template name \\nand batch name [get that from source template workbench(unique) ]', '', ' \\n - User Manual  | Confidential  Page | 15', '• Click on the transform to validate and convert.  \\n• Once done , ConvertRite  application needs to be connected to oracle fusion  to run the jobs.   \\n➢ Re-Process option is used to re -validate only failed records after correcting the errors.  \\n➢ FBDI – File based data import - is the file generated after the conver sions in our application before \\nmoving into the cloud.  \\n➢ FBDI – is only for ERP modelling, HDL – is for HCM related data. ', 'Step 1 4:    \\nLoad Import Metadata:  \\n• So, click on the hamburger button -Go to the Master  Data option under  – Load Import Metadata – fill the \\ninformation provided by  the respective  functional team – Save.', 'Step 1 5:      \\nCloud Load Import:  \\n• Click on the hamburger button - ConvertRite - Cloud Load Import – select  template name , batch name, \\nparameter list –based on the parent object  level. ', '• Click on the Load Import – request will be submitted,  and result ID should be generated. ', '', ' \\n - User Manual  | Confidential  Page | 16 \\n➢ Recon cile – When we click this button, it hits the template and compares.  \\n➢ Reconcile Report – Generates all the reports Pass/Fail . \\n➢ Rejection Report  – Only the details of rejected results in reconcile.  \\nReconciliation  \\nData reconciliation (DR) is a term typically used to describe  a verific ation phase during a data migration \\nwhere the target data is compared against original source data to ensure that the migration architecture has \\ntransferred the data correctly.  Reconciliation is the process of ensuring that two sets of records are in \\nagree ment.   \\n• Click on the hamburger button - Dash Boards – Reconcile – select object,  cloud template,  cloud  batch - \\nview. ', '• We will get the results of source data extracted, validated and data migrated to the cloud . \\n• We can check error records and details.  \\nOther  Screens  \\nLoad Cockpit:  \\nLet’s say object and the flow is already created – to load the data - transform and cloud data loading for the \\nsame parent object with different child objects in the same screen.  \\n• We can use Load Cockpit option from the Hamburger button - Automation - Load Cockpit – New – fill up all \\nthe information.  \\n• Cloud and Source templates should be created/available to use Load Cockpit option. ', '• Click on Load Source Data button – same functionality as to Load Data from Source Template \\nWorkbench. ', '', ' \\n - User Manual  | Confidential  Page | 17 \\n• Enter the U nique Batch Name.  \\n• Click on the T ransform to validate all mapping of Cloud Template Workbench screen. ', '• If you want to upload file manually, choose manual radio button . \\n• If the  file-based data is different for all the object s– then upload the file from Load Source ( Denormalize ) \\nand if file based data is same for all the objects then File can be uploaded from load source (Normalise) . \\nSource Conversions workbench:  \\n• Click on the Hamburger button - ConvertRite – Source Conversions Workbench - select the template to \\ncheck  failed records. ', '• In this screen we can check  and access  failed records when we are extracting/loading source data . \\n• Download option under status will be enabled only when we use external option to upload file in the \\nsource template workb ench .', '', '', ' - User Manual  | Confidential  Page | 18 \\nHCM Load Import:  \\n• Click on the hamburger button  - ConvertRite – HCM Load import – New –fill all required details  - Load \\nand Import. ', '• HCM Load Import screen is used to migrate data directly into cloud when we are dealing with HCM object \\ncodes.  \\n• Summary Button hits the summary of the cloud table and Summary report generates the file with \\nsummary data.  \\nDate Configuration:  \\n• Click on the hamburger button – Configuration – Date Configuration  - New- Fill details - Save.', '• Date configuration screen is used to change the source date format before converting the data as the \\nrequired date format in cloud.  \\nFBDI Workbench (XLSM):  \\n• Click on the hamburger button – Sequence Generator/Grouping – FBDI Workbench (X lsm) -New – Save. ', '• FBDI Workbench ( Xlsm) screen is used to generate sequence based on the xlsm file in  the cloud \\ndatabase.  \\n➢ Download comments – downloads the column details with description.  \\n➢ Save as Blob – History of object sequence with version will be saved for future reference. ', '', ' \\n - User Manual  | Confidential  Page | 19 \\nObject Grouping:  \\n• Click on the hamburger button - Sequence Generator/Grouping – Object Grouping . \\n• Object grouping is used to group the objects based on Excel File of Cloud. ', '• If we do Objec t grouping, template grouping should be done.  \\nTemplate Grouping:  \\n• Click on  the hamburger button - Sequence Generator/Grouping –Template Grouping .', '• Template Grouping screen is used to group the same objects that are grouped in  the object grouping by \\nselecting template.  \\nCloud Sql Adhoc Query:  \\n• Click on the hamburger button – Master data – Cloud SQL Adhoc Query  –New – Write SQL query . ', '• This screen is used to get the lookups from cloud with SQL query.  \\n• Check the Update cloud lookup data box to update data in the lookup.  \\nLoad Cockpit – HCM:  \\n• Click on the hamburger button - Automation  – Load Cockpit HCM  – fill the required info – Load Source \\nData (Auto) – Transform – HCM Cloud Import .', '', ' \\n - User Manual  | Confidential  Page | 20', '• This screen is used to do the process in on e screen for HCM object codes.  \\nStatus of Jobs:  \\n• Click on  the hamburger button -Dash Boards – Status of jobs .', ' Help:  \\n• Click on  the hamburger button – Dash Boards – Help. \\n.  \\n ', '', ' \\n - User Manual  | Confidential  Page | 21 \\nAbbreviations  \\nAbbreviation  Full Form  \\nERP Enterprise Resource Planning  \\nPOD  Product Oriented Delivery  \\nHDL HCM Data Loader  \\nFBDI  File Based Data Import  \\nOrig Trans Ref Original Transaction References  \\nBU Business Unit ', '', '', '', ' \\n ']\n"
     ]
    }
   ],
   "source": [
    "words = nltk.word_tokenize(pdf_text)\n",
    "\n",
    "# Chunk the words into segments of 100 words each, with 50-word overlap\n",
    "chunk_size = 10\n",
    "overlap_size = 0\n",
    "chunks = []\n",
    "paragraphs = pdf_text.split(' \\n \\n')\n",
    "chunks = pdf_text.split(' \\n \\n')\n",
    "tempchunks=[]\n",
    "# for i in range(0, len(words), chunk_size - overlap_size):\n",
    "#     chunk = words[i:i + chunk_size]\n",
    "#     tempchunks.append(chunk)\n",
    "# print(tempchunks)\n",
    "chunks_final=[]\n",
    "# print(chunks)\n",
    "# print(\"\\n\\n\")\n",
    "# Print the chunks\n",
    "for i in chunks:\n",
    "    # print(i)\n",
    "    # print(\"\\n\\n\")\n",
    "    chunks_final.append(''.join(i))\n",
    "print(chunks_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "User Manual \n",
      "Table of Contents  \n",
      "Introduction       3 \n",
      "High Level Process Flow       3 \n",
      "Login Information       3 \n",
      "Workflow       4 \n",
      "Reconciliation        16 \n",
      "Other  Screens       16 \n",
      "Abbreviations        21 \n",
      " \n",
      "Introduction  \n",
      "It is a data conversion tool built on Oracle technologies designed to automate data conversion from any \n",
      "source to Oracle Cloud Applications With ConvertRite you can automate manual time consuming and \n",
      "error prone processes such as data mapping and validation and convert all your data from legacy \n",
      "applications to make it compatible with Oracle Cloud Applications  \n",
      "High Level Process Flow\n",
      "Login Information  \n",
      "Link   \n",
      "Login The user should log in and select the role of admin to access ConvertRite   \n",
      "Workflow  \n",
      "Click on hamburger menu  on top left \n",
      "for application navigation  \n",
      "      \n",
      " \n",
      " Project POD Parent object and child object  object code  are \n",
      "Master data  \n",
      "STEP 1  \n",
      "Creation  of POD  \n",
      "POD  The Product Oriented Delivery POD model is  a software development strategy that centers on \n",
      "building small cross functional teams that own specific tasks or requirements for a project  \n",
      "A project can be created in different PODs or many Projects in one POD can be \n",
      "created  \n",
      " To create  a POD in ConvertRite click on hamburger button  master data  \n",
      "lookup workbench  search for POD  click on add at lookup values   add POD \n",
      "name  POD  is independent  doesnt rel y on any other values  save\n",
      " Export CSV  Export CSV button downloads all the file based data on this \n",
      "particular screen \n",
      "Creation of Project  \n",
      " To create the project in the lookup header  search for  the Lookup name as Project and add the Lookup \n",
      "value  your project details \n",
      " Project is related to POD  Hence select the required POD   Save \n",
      "Creation of Parent Object  \n",
      " Click on the hamburger button  Master data  Lookup workbench   \n",
      " Click on Search  Search for Parent object  add a lookup value  define lookup name as required parent \n",
      "object from FBDI file  Create parent obj ect  Assign project name in actual value \n",
      " If the project is not listed under actual value need to add project details under project name lookup name \n",
      "Creation of Child Object  \n",
      " Search for lookup name  object  code   add the lookup value for child object from FBDI file  which should \n",
      "be related to  the parent object code \n",
      " \n",
      "Step 2 \n",
      "Assign POD  and Project  \n",
      " Click on the Hamburger button ConvertRite   Manage your project  New \n",
      " Define project name POD and all mandatory information  save  POD environment will be assigned to  the \n",
      "project and object in this screen  \n",
      "         \n",
      " Copy project is used to copy the complete project with same data into a different POD with project \n",
      "status start and completion date \n",
      "\n",
      " Click on Load WBS  Project will be assigned to parent object \n",
      " If the project is created already search for the required project and Load WBS it can be done for the \n",
      "other parent object Load WBS is parent object level  \n",
      "Step 3  \n",
      "Assign Role Object  \n",
      " We must  Enable flag  to activate the  load metadata option   \n",
      " Click on the Admin  switch to SuperUser\n",
      " \n",
      " In the SuperUser profile click on the hamburger button on top left  admin  role object \n",
      "\n",
      " Select the role name as admin required POD Project  parent object  Enable flag Once done switch \n",
      "back to admin \n",
      " Master data configuration done  \n",
      " We must configure and connect cloudsource to database to get metadata so that stagging table can be \n",
      "created  \n",
      " On cloud side after completing configuration  load metadata should be done on source  side  both can \n",
      "be done on same screen  \n",
      "Step 4  \n",
      "Source  Configuration  Parent object level  \n",
      " Click on the Hamburger button  Configuration  External Source Configuration  New  fill up info rmation  \n",
      " Save  Parent object level  Upload file View file provided by functional team \n",
      " If parent object is available we can use update view option to update any details  To update view file for \n",
      "each individual child object  \n",
      " Click on the load metadata  structure is done \n",
      "Step 5  \n",
      "Source Template workbench  \n",
      " Click on the Hamburger button ConvertRite  Source Template Workbench  New  fill all required fields  \n",
      "Save Create Table  Import Columns     \n",
      " BU Specific must  be enabled assign the template at business user level  \n",
      " Check the Normalize box to differentiate duplicate data Process will be stopped if any duplicate data \n",
      "is identified  \n",
      " Select all columns displayed  Save \n",
      " Click on the Baseline  Stagging table name and view name will be displayed if we click on three dots \n",
      "Stagging table will have all the data but view have only required data  \n",
      " Select Ori g trans ref  To link source and cloud columns data  unique identification for the records  and \n",
      "also when  we n eed 2 or more columns data to merge the data to 1 column in cloud  \n",
      " Re Orig  Tras ref is used when an Orig trans ref is already created and requirement is \n",
      "changedupdated  we can update re Orig trans based on the batch name  \n",
      " Load Data  success make a note  of the batch name  unique name should be given by us \n",
      " While loading data choose manual  to upload small data through file and choose external option to \n",
      "upload large data \n",
      "\n",
      "Step 6 \n",
      "Cloud Access Configuration  \n",
      " Click on  the Hamburger button  Configuration   Cloud Access configuration  \n",
      " Cloud Access Configuration screen is used to connect to  the cloud SaaS environment with the required \n",
      "credentials  \n",
      " Cloud URL  SaaS URL and credentials will be provided by the functional team \n",
      "Step 7  \n",
      "Cloud Configuration  \n",
      " Click on the Hamburger button  Configuration  Cloud Configuration  \n",
      " Click on New Define all re quired  info From FBDI  Object Code  Ctrl File Name  Xlsm F ile Name \n",
      "Sheet Name Interface Table\n",
      " Hence Connected to cloud We must get the structure  through load meta data step  Only after  role \n",
      "object parent object level \n",
      "Step 8  \n",
      "Cloud Load Metadata  \n",
      " Click on the Hamburger button Master Data Load metadata  select Cloud  Cloud Load Metadata \n",
      " On the pop  up screen select all the required data   Metadata table will be created \n",
      " We can load metadata to source by selecting Source radio  click on EBS adapter  \n",
      " By Clicking on EBS adapter it will re direct to external source co nfiguration screen  where we can \n",
      "select particular parent object to create metadata  click on upload file and load data  \n",
      "Step 9 \n",
      "Cloud Stagging table  \n",
      " Click on  the Hamburger button   ConvertRite   Cloud Template Workbench  New  fill all required \n",
      "information  Save  Create Table  Import Columns \n",
      " In the source template field the new template we have  created should be available  select that source \n",
      "template  and SAVE  \n",
      "\n",
      " \n",
      " Source columns will be added and listed on this screen  \n",
      " On the Cloud Template Workbench  click on the anchor icon  on top right which is called user hooks \n",
      "Extraction validation  transformation and cloud import \n",
      " \n",
      " We can enable pre or post hook to manipulate any data before or afte r validation  \n",
      " Hook value  Any SQL query  As provided  \n",
      "Step 10  \n",
      "Sequence Generator  \n",
      " Now to generate sequence  go to hamburger button  Sequence Generator Grouping  FBDI Workbench \n",
      "Ctrl  New  Fill all the details   choose Auto   Save \n",
      " Note We can generate sequence using Ctrl file FBDI workbench ctrl   or  Xlsm file FBDI workbench \n",
      "xlsm\n",
      " Switch back to Cloud Template Workbench  Seq ctrl button will be enabled click on it and sort \n",
      "sequence \n",
      "\n",
      "Step 11   \n",
      " We must do the mapping after generating sequence  information will be provided by sourcefunctional \n",
      "team  \n",
      "Mapping types   \n",
      "1 As  Is  Moves X  X \n",
      "2 Prefix   Add X   Before data value  \n",
      "3 Suffix   Add   X After the data value  \n",
      "4 Constant   Source no v alue but should sent constant value to cloud  \n",
      "5 One to One   One field of source will be mapped to one cloud value  \n",
      "6 Two to One   Two fields of source will be mapped to one cloud value  \n",
      "7 Three to One   Three fields of source will be mapped to one cloud value  \n",
      "8 Formula set  Write query to do other conversions optional values  \n",
      " Click on the hamburger button  Data Transformation  Define Mapping Set To Define  11 21 31 New \n",
      "define all information and save  \n",
      " Once the mapping set is saved add mapping set value s column  column \n",
      " Source Object   View file  from the source template  \n",
      " Source Column  Column name in the source side  \n",
      " Source Field  Column value in the source side  \n",
      " Cloud Column  Column name from the cloud s ide This  information will be provided by functional  team  \n",
      " Cloud Value Column  value to be updated on the cloud side \n",
      " We must  write the condition for 1 1 21 and 3 1 mappings in W here Clause \n",
      " SQL query will be generated by the application  on the backend  and hence transforms the data  \n",
      "Define Mapping Set \n",
      " To define any other mapping type to transform the data   Click on menu  Data Transformation  Define \n",
      "Formula Set we can write required SQL query  \n",
      " We have 3 Formula types  Java SQL and function  \n",
      " Test SQL is used to test the SQL query written by us  \n",
      " Test Data  is used to test the data with given SQL query at Orig trans level  \n",
      " Original Trans Ref  The records that we select as org trans ref in  the source side  \n",
      " Formula value  Expected value should be given \n",
      " SQL Query  Should be written on Org trans ref  \n",
      " Test SQL  To test the SQL query written  \n",
      " Test Data  To test the data that we got by the sql query written \n",
      "Step 1 2 \n",
      "In Cloud template workbench  \n",
      " Select all the columns for which sequence is generated  Save  Click on Baseline to create table \n",
      "Step 1 3      \n",
      "Validation  \n",
      " Click on the Hamburger  button  ConvertRite  Cloud Conversions Workbench  select  the template name \n",
      "and batch name get that from source template workbenchunique \n",
      "\n",
      " Click on the transform to validate and convert  \n",
      " Once done  ConvertRite  application needs to be connected to oracle fusion  to run the jobs   \n",
      " ReProcess option is used to re validate only failed records after correcting the errors  \n",
      " FBDI  File based data import  is the file generated after the conver sions in our application before \n",
      "moving into the cloud  \n",
      " FBDI  is only for ERP modelling HDL  is for HCM related data \n",
      "Step 1 4    \n",
      "Load Import Metadata  \n",
      " So click on the hamburger button Go to the Master  Data option under   Load Import Metadata  fill the \n",
      "information provided by  the respective  functional team  Save\n",
      "Step 1 5      \n",
      "Cloud Load Import  \n",
      " Click on the hamburger button  ConvertRite  Cloud Load Import  select  template name  batch name \n",
      "parameter list based on the parent object  level \n",
      " Click on the Load Import  request will be submitted  and result ID should be generated \n",
      " Recon cile  When we click this button it hits the template and compares  \n",
      " Reconcile Report  Generates all the reports PassFail  \n",
      " Rejection Report   Only the details of rejected results in reconcile  \n",
      "Reconciliation  \n",
      "Data reconciliation DR is a term typically used to describe  a verific ation phase during a data migration \n",
      "where the target data is compared against original source data to ensure that the migration architecture has \n",
      "transferred the data correctly  Reconciliation is the process of ensuring that two sets of records are in \n",
      "agree ment   \n",
      " Click on the hamburger button  Dash Boards  Reconcile  select object  cloud template  cloud  batch  \n",
      "view \n",
      " We will get the results of source data extracted validated and data migrated to the cloud  \n",
      " We can check error records and details  \n",
      "Other  Screens  \n",
      "Load Cockpit  \n",
      "Lets say object and the flow is already created  to load the data  transform and cloud data loading for the \n",
      "same parent object with different child objects in the same screen  \n",
      " We can use Load Cockpit option from the Hamburger button  Automation  Load Cockpit  New  fill up all \n",
      "the information  \n",
      " Cloud and Source templates should be createdavailable to use Load Cockpit option \n",
      " Click on Load Source Data button  same functionality as to Load Data from Source Template \n",
      "Workbench \n",
      " Enter the U nique Batch Name  \n",
      " Click on the T ransform to validate all mapping of Cloud Template Workbench screen \n",
      " If you want to upload file manually choose manual radio button  \n",
      " If the  filebased data is different for all the object s then upload the file from Load Source  Denormalize  \n",
      "and if file based data is same for all the objects then File can be uploaded from load source Normalise  \n",
      "Source Conversions workbench  \n",
      " Click on the Hamburger button  ConvertRite  Source Conversions Workbench  select the template to \n",
      "check  failed records \n",
      " In this screen we can check  and access  failed records when we are extractingloading source data  \n",
      " Download option under status will be enabled only when we use external option to upload file in the \n",
      "source template workb ench \n",
      "HCM Load Import  \n",
      " Click on the hamburger button   ConvertRite  HCM Load import  New fill all required details   Load \n",
      "and Import \n",
      " HCM Load Import screen is used to migrate data directly into cloud when we are dealing with HCM object \n",
      "codes  \n",
      " Summary Button hits the summary of the cloud table and Summary report generates the file with \n",
      "summary data  \n",
      "Date Configuration  \n",
      " Click on the hamburger button  Configuration  Date Configuration   New Fill details  Save\n",
      " Date configuration screen is used to change the source date format before converting the data as the \n",
      "required date format in cloud  \n",
      "FBDI Workbench XLSM  \n",
      " Click on the hamburger button  Sequence GeneratorGrouping  FBDI Workbench X lsm New  Save \n",
      " FBDI Workbench  Xlsm screen is used to generate sequence based on the xlsm file in  the cloud \n",
      "database  \n",
      " Download comments  downloads the column details with description  \n",
      " Save as Blob  History of object sequence with version will be saved for future reference \n",
      "Object Grouping  \n",
      " Click on the hamburger button  Sequence GeneratorGrouping  Object Grouping  \n",
      " Object grouping is used to group the objects based on Excel File of Cloud \n",
      " If we do Objec t grouping template grouping should be done  \n",
      "Template Grouping  \n",
      " Click on  the hamburger button  Sequence GeneratorGrouping Template Grouping \n",
      " Template Grouping screen is used to group the same objects that are grouped in  the object grouping by \n",
      "selecting template  \n",
      "Cloud Sql Adhoc Query  \n",
      " Click on the hamburger button  Master data  Cloud SQL Adhoc Query  New  Write SQL query  \n",
      " This screen is used to get the lookups from cloud with SQL query  \n",
      " Check the Update cloud lookup data box to update data in the lookup  \n",
      "Load Cockpit  HCM  \n",
      " Click on the hamburger button  Automation   Load Cockpit HCM   fill the required info  Load Source \n",
      "Data Auto  Transform  HCM Cloud Import \n",
      "\n",
      " This screen is used to do the process in on e screen for HCM object codes  \n",
      "Status of Jobs  \n",
      " Click on  the hamburger button Dash Boards  Status of jobs \n",
      " Help  \n",
      " Click on  the hamburger button  Dash Boards  Help \n",
      "  \n",
      " \n",
      "Abbreviations  \n",
      "Abbreviation  Full Form  \n",
      "ERP Enterprise Resource Planning  \n",
      "POD  Product Oriented Delivery  \n",
      "HDL HCM Data Loader  \n",
      "FBDI  File Based Data Import  \n",
      "Orig Trans Ref Original Transaction References  \n",
      "BU Business Unit \n",
      "65\n"
     ]
    }
   ],
   "source": [
    "chunks_final = [chunk for chunk in chunks_final if chunk.strip()]\n",
    "\n",
    "# Remove stop words\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# words = word_tokenize(text)\n",
    "# for i in chunks_final:\n",
    "    # chunks_final = [i.lower() for i in chunks_final if i.lower() not in stop_words]\n",
    "\n",
    "# Remove punctuation and URLs\n",
    "chunks_final = [re.sub(r\"^\\s*-\\s*User Manual\\s*\\|\\s*Confidential\\s*Page\\s*\\|\\s*\\d+\\s*\", \"\", chunk)for chunk in chunks_final]\n",
    "chunks_final = [re.sub(r'[^\\w\\s]', '', chunk) for chunk in chunks_final]\n",
    "chunks_final = [re.sub(r'http\\S+', '', chunk) for chunk in chunks_final]\n",
    "\n",
    "# # Tokenization\n",
    "# tokens = word_tokenize(\" \".join(words))\n",
    "\n",
    "# # Stemming\n",
    "# stemmer = PorterStemmer()\n",
    "# stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "\n",
    "# # Lemmatization\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "for chunk in chunks_final:\n",
    "    print(chunk)\n",
    "print(len(chunks_final))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2385\n"
     ]
    }
   ],
   "source": [
    "word_count=0\n",
    "for i in chunks_final:\n",
    "\n",
    "    word_count+=len(i.split())\n",
    "    \n",
    "print(word_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \\nUser Manual Table of Contents  \\nIntroduction       3 \\nHigh Level Process Flow       3 \\nLogin Information       3 \\nWorkflow       4 \\nReconciliation        16 \\nOther  Screens       16 \\nAbbreviations        21 \\n ', 'Creation of Project  \\n To create the project in the lookup header  search for  the Lookup name as Project and add the Lookup \\nvalue  your project details  Project is related to POD  Hence select the required POD   Save ', ' Click on Load WBS  Project will be assigned to parent object  If the project is created already search for the required project and Load WBS it can be done for the \\nother parent object Load WBS is parent object level  \\nStep 3  \\nAssign Role Object  \\n We must  Enable flag  to activate the  load metadata option   \\n Click on the Admin  switch to SuperUser', 'Step 5  \\nSource Template workbench  \\n Click on the Hamburger button ConvertRite  Source Template Workbench  New  fill all required fields  \\nSave Create Table  Import Columns      BU Specific must  be enabled assign the template at business user level  \\n Check the Normalize box to differentiate duplicate data Process will be stopped if any duplicate data \\nis identified  \\n Select all columns displayed  Save \\n Click on the Baseline  Stagging table name and view name will be displayed if we click on three dots \\nStagging table will have all the data but view have only required data  \\n Select Ori g trans ref  To link source and cloud columns data  unique identification for the records  and \\nalso when  we n eed 2 or more columns data to merge the data to 1 column in cloud  \\n Re Orig  Tras ref is used when an Orig trans ref is already created and requirement is \\nchangedupdated  we can update re Orig trans based on the batch name  \\n Load Data  success make a note  of the batch name  unique name should be given by us ', 'Step 8  \\nCloud Load Metadata  \\n Click on the Hamburger button Master Data Load metadata  select Cloud  Cloud Load Metadata  On the pop  up screen select all the required data   Metadata table will be created ', ' Switch back to Cloud Template Workbench  Seq ctrl button will be enabled click on it and sort \\nsequence ', ' Click on the transform to validate and convert  \\n Once done  ConvertRite  application needs to be connected to oracle fusion  to run the jobs   \\n ReProcess option is used to re validate only failed records after correcting the errors  \\n FBDI  File based data import  is the file generated after the conver sions in our application before \\nmoving into the cloud  \\n FBDI  is only for ERP modelling HDL  is for HCM related data ', ' Click on Load Source Data button  same functionality as to Load Data from Source Template \\nWorkbench  Enter the U nique Batch Name  \\n Click on the T ransform to validate all mapping of Cloud Template Workbench screen ', ' FBDI Workbench  Xlsm screen is used to generate sequence based on the xlsm file in  the cloud \\ndatabase  \\n Download comments  downloads the column details with description  \\n Save as Blob  History of object sequence with version will be saved for future reference Object Grouping  \\n Click on the hamburger button  Sequence GeneratorGrouping  Object Grouping  \\n Object grouping is used to group the objects based on Excel File of Cloud ', ' Help  \\n Click on  the hamburger button  Dash Boards  Help \\n  \\n Abbreviations  \\nAbbreviation  Full Form  \\nERP Enterprise Resource Planning  \\nPOD  Product Oriented Delivery  \\nHDL HCM Data Loader  \\nFBDI  File Based Data Import  \\nOrig Trans Ref Original Transaction References  \\nBU Business Unit ']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "# Define the original list of 58 strings\n",
    "# original_list = [\"string1\", \"string2\", \"string3\", ..., \"string57\", \"string58\"]\n",
    "\n",
    "# Create a new list of 10 strings by selecting every 6th string\n",
    "chunks_final = [chunks_final[i] + chunks_final[i+1] for i in range(0, len(chunks_final), math.ceil(len(chunks_final)/10))]\n",
    "\n",
    "# Print the new list of 10 strings\n",
    "print(chunks_final)\n",
    "print(len(chunks_final))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "User Manual \n",
      " - User Manual  | Confidential  Page | 2 \n",
      "Table of Contents  \n",
      "Introduction ................................ ................................ ................................ ................................ ..............  3 \n",
      "High Level Process Flow  ................................ ................................ ................................ .........................  3 \n",
      "Login Information  ................................ ................................ ................................ ................................ .... 3 \n",
      "Workflow ................................ ................................ ................................ ................................ ..................  4 \n",
      "Reconciliation  ................................ ................................ ................................ ................................ ........  16 \n",
      "Other  Screens  ................................ ................................ ................................ ................................ ....... 16 \n",
      "Abbreviations  ................................ ................................ ................................ ................................ ........  21 \n",
      " \n",
      " - User Manual  | Confidential  Page | 3 \n",
      "Introduction  \n",
      "It is a data conversion tool built on Oracle technologies, designed to automate data conversion from any \n",
      "source to Oracle Cloud Applications. With ConvertRite, you can automate manual, time -consuming and \n",
      "error -prone processes (such as data mapping and validation) and convert all your data from legacy \n",
      "applications to make it compatible with Oracle Cloud Applications.  \n",
      "High Level Process Flow\n",
      "Login Information  \n",
      "Link: https://convertrite.ritesoftware.com/  \n",
      "Login: The user should log in and select the role of admin to access ConvertRite.   \n",
      " \n",
      " - User Manual  | Confidential  Page | 4 \n",
      "Workflow  \n",
      "Click on hamburger menu  on top left \n",
      "for application navigation.  \n",
      "      \n",
      " \n",
      "➢ Project, POD, Parent object and child object  (object code)  are \n",
      "Master data.  \n",
      "STEP 1:  \n",
      "Creation  of POD:  \n",
      "POD - The Product Oriented Delivery (POD) model is  a software development strategy that centers on \n",
      "building small cross -functional teams that own specific tasks or requirements for a project.  \n",
      "A project can be created in different PODs or many Projects in one POD can be \n",
      "created.  \n",
      "• To create  a POD in ConvertRite, click on hamburger button - master data - \n",
      "lookup workbench - search for POD – click on add at lookup values  – add POD \n",
      "name.  (POD  is independent - doesn’t rel y on any other values) – save.\n",
      "➢ Export CSV – Export CSV button downloads all the file based data on this \n",
      "particular screen. \n",
      "Creation of Project : \n",
      "• To create the project, in the lookup header – search for  the Lookup name as Project and add the Lookup \n",
      "value – your project details. \n",
      " \n",
      " - User Manual  | Confidential  Page | 5 \n",
      "• Project is related to POD – Hence select the required POD  – Save .\n",
      "Creation of Parent Object:  \n",
      "• Click on the hamburger button - Master data - Lookup workbench .  \n",
      "• Click on Search  -Search for Parent object - add a lookup value - define lookup name as required parent \n",
      "object from FBDI file ( Create parent obj ect) – Assign project name in actual value .\n",
      "• If the project is not listed under actual value, need to add project details under project name lookup name. \n",
      " \n",
      " - User Manual  | Confidential  Page | 6 \n",
      "Creation of Child Object:  \n",
      "• Search for lookup name – object  code  – add the lookup value for child object from FBDI file  which should \n",
      "be related to  the parent object code. \n",
      " \n",
      "Step 2: \n",
      "Assign POD  and Project : \n",
      "• Click on the Hamburger button –ConvertRite  – Manage your project – New. \n",
      "• Define project name, POD and all mandatory information - save  (POD environment will be assigned to  the \n",
      "project and object in this screen) . \n",
      "         \n",
      "➢ Copy project is used to copy the complete project with same data into a different POD with project \n",
      "status, start and completion date. \n",
      " \n",
      " - User Manual  | Confidential  Page | 7\n",
      "• Click on Load WBS – Project will be assigned to parent object. \n",
      "• If the project is created already, search for the required project and Load WBS [it can be done for the \n",
      "other parent object (Load WBS is parent object level) ] \n",
      "Step 3:  \n",
      "Assign Role Object:  \n",
      "• We must  Enable flag  to activate the  load metadata option.   \n",
      "• Click on the Admin – switch to SuperUser.\n",
      " \n",
      "• In the SuperUser profile, click on the hamburger button on top left - admin – role object. \n",
      " - User Manual  | Confidential  Page | 8\n",
      "• Select the role name as admin, required POD, Project,  parent object – Enable flag. Once done, switch \n",
      "back to admin. \n",
      "✓ Master data configuration done . \n",
      "• We must configure and connect cloud/source to database to get metadata so that stagging table can be \n",
      "created.  \n",
      "• On cloud side after completing configuration – load metadata should be done, on source  side – both can \n",
      "be done on same screen.  \n",
      "Step 4:  \n",
      "Source  Configuration:  (Parent object level)  \n",
      "• Click on the Hamburger button - Configuration – External Source Configuration – New – fill up info rmation  \n",
      "– Save ( Parent object level) – Upload file (View file provided by functional team). \n",
      "• If parent object is available, we can use update view option to update any details . (To update view file for \n",
      "each individual child object) . \n",
      "• Click on the load metadata – structure is done .\n",
      " \n",
      " - User Manual  | Confidential  Page | 9 \n",
      "Step 5:  \n",
      "Source Template workbench:  \n",
      "• Click on the Hamburger button -ConvertRite - Source Template Workbench - New – fill all required fields – \n",
      "Save- Create Table – Import Columns.     \n",
      "➢ BU Specific must  be enabled assign the template at business user level.  \n",
      "➢ Check the Normalize box to differentiate duplicate data (Process will be stopped if any duplicate data \n",
      "is identified) . \n",
      "• Select all columns displayed - Save. \n",
      "• Click on the Baseline – Stagging table name and view name will be displayed if we click on three dots \n",
      "(Stagging table will have all the data but view have only required data) . \n",
      "• Select Ori g trans ref - To link source and cloud columns data , unique identification for the records  and \n",
      "also when  we n eed 2 or more columns data to merge the data to 1 column in cloud . \n",
      "➢ Re Orig  Tras ref is used when an Orig trans ref is already created and requirement is \n",
      "changed/updated - we can update re Orig trans based on the batch name . \n",
      "• Load Data – success (make a note  of the batch name – unique name should be given by us) .\n",
      "➢ While loading data, choose manual  to upload small data through file and choose external option to \n",
      "upload large data. \n",
      " \n",
      " - User Manual  | Confidential  Page | 10\n",
      "Step 6: \n",
      "Cloud Access Configuration:  \n",
      "• Click on  the Hamburger button – Configuration  – Cloud Access configuration.  \n",
      "• Cloud Access Configuration screen is used to connect to  the cloud SaaS environment with the required \n",
      "credentials.  \n",
      "• Cloud URL – SaaS URL and credentials will be provided by the functional team. \n",
      "Step 7:  \n",
      "Cloud Configuration:  \n",
      "• Click on the Hamburger button – Configuration – Cloud Configuration . \n",
      "• Click on New –Define all re quired  info (From FBDI ): Object Code – Ctrl File Name – Xlsm F ile Name -\n",
      "Sheet Name- Interface Table.\n",
      "• Hence Connected to cloud. We must get the structure – through load meta data step. ( Only after  role \n",
      "object -parent object level) .\n",
      " \n",
      " - User Manual  | Confidential  Page | 11 \n",
      "Step 8:  \n",
      "Cloud Load Metadata:  \n",
      "• Click on the Hamburger button -Master Data- Load metadata - select Cloud – Cloud Load Metadata .\n",
      "• On the pop - up screen, select all the required data  – Metadata table will be created. \n",
      "➢ We can load metadata to source by selecting Source radio - click on EBS adapter.  \n",
      "➢ By Clicking on EBS adapter, it will re -direct to external source co nfiguration screen  where we can \n",
      "select particular parent object to create metadata , click on upload file and load data.  \n",
      "Step 9: \n",
      "Cloud Stagging table:  \n",
      "• Click on  the Hamburger button  - ConvertRite  – Cloud Template Workbench – New – fill all required \n",
      "information - Save – Create Table – Import Columns. \n",
      "• In the source template field- the new template we have  created should be available,  select that source \n",
      "template  and SAVE . \n",
      " - User Manual  | Confidential  Page | 12\n",
      " \n",
      "• Source columns will be added and listed on this screen . \n",
      "• On the Cloud Template Workbench – click on the anchor icon ( on top right) which is called user hooks \n",
      "(Extraction, validation & transformation, and cloud import) .\n",
      " \n",
      "• We can enable pre or post hook to manipulate any data before or afte r validation . \n",
      "• Hook value = Any SQL query ( As provided) . \n",
      "Step 10:  \n",
      "Sequence Generator:  \n",
      "• Now to generate sequence – go to hamburger button - Sequence Generator/ Grouping – FBDI Workbench \n",
      "(Ctrl) – New - Fill all the details  – choose Auto  – Save. \n",
      "• Note: We can generate sequence using Ctrl file [FBDI workbench (ctrl)]   or  Xlsm file [FBDI workbench \n",
      "(xlsm)]\n",
      "• Switch back to Cloud Template Workbench – Seq+ ctrl button will be enabled, click on it and sort \n",
      "sequence .\n",
      " \n",
      " - User Manual  | Confidential  Page | 13\n",
      "Step 11:   \n",
      "• We must do the mapping (after generating sequence) – information will be provided by source/functional \n",
      "team.  \n",
      "Mapping types:   \n",
      "1. As – Is - Moves X - X \n",
      "2. Prefix  - Add X - …... (Before data value)  \n",
      "3. Suffix  - Add …... - X (After the data value)  \n",
      "4. Constant  - Source no v alue but should sent constant value to cloud  \n",
      "5. One to One  - One field of source will be mapped to one cloud value  \n",
      "6. Two to One  - Two fields of source will be mapped to one cloud value  \n",
      "7. Three to One  - Three fields of source will be mapped to one cloud value  \n",
      "8. Formula set - Write query to do other conversions (optional values)  \n",
      "• Click on the hamburger button - Data Transformation - Define Mapping Set (To Define  1-1, 2-1, 3-1) -New- \n",
      "define all information and save.  \n",
      "• Once the mapping set is saved, add mapping set value s (column – column) .\n",
      "• Source Object  - View file  from the source template  \n",
      "• Source Column – Column name in the source side  \n",
      "• Source Field – Column value in the source side  \n",
      "• Cloud Column – Column name from the cloud s ide (This  information will be provided by functional  team ) \n",
      "• Cloud Value –Column  value to be updated on the cloud side \n",
      "• We must  write the condition for 1 -1, 2-1 and 3 -1 mappings in W here Clause .\n",
      " \n",
      " - User Manual  | Confidential  Page | 14 \n",
      "• SQL query will be generated by the application  on the backend  and hence transforms the data . \n",
      "Define Mapping Set: \n",
      "• To define any other mapping type to transform the data  - Click on menu - Data Transformation - Define \n",
      "Formula Set –we can write required SQL query.  \n",
      "• We have 3 Formula types – Java, SQL and function . \n",
      "• Test SQL is used to test the SQL query written by us.  \n",
      "• Test Data  is used to test the data with given SQL query at Orig trans level.  \n",
      "• Original Trans Ref – The records that we select as org trans ref in  the source side . \n",
      "• Formula value – Expected value should be given. \n",
      "• SQL Query – Should be written on Org trans ref . \n",
      "• Test SQL – To test the SQL query written . \n",
      "• Test Data – To test the data that we got by the sql query written. \n",
      "Step 1 2: \n",
      "In Cloud template workbench:  \n",
      "• Select all the columns for which sequence is generated – Save - Click on Baseline to create table. \n",
      "Step 1 3:      \n",
      "Validation : \n",
      "• Click on the Hamburger  button - ConvertRite - Cloud Conversions Workbench – select  the template name \n",
      "and batch name [get that from source template workbench(unique) ]\n",
      " \n",
      " - User Manual  | Confidential  Page | 15\n",
      "• Click on the transform to validate and convert.  \n",
      "• Once done , ConvertRite  application needs to be connected to oracle fusion  to run the jobs.   \n",
      "➢ Re-Process option is used to re -validate only failed records after correcting the errors.  \n",
      "➢ FBDI – File based data import - is the file generated after the conver sions in our application before \n",
      "moving into the cloud.  \n",
      "➢ FBDI – is only for ERP modelling, HDL – is for HCM related data. \n",
      "Step 1 4:    \n",
      "Load Import Metadata:  \n",
      "• So, click on the hamburger button -Go to the Master  Data option under  – Load Import Metadata – fill the \n",
      "information provided by  the respective  functional team – Save.\n",
      "Step 1 5:      \n",
      "Cloud Load Import:  \n",
      "• Click on the hamburger button - ConvertRite - Cloud Load Import – select  template name , batch name, \n",
      "parameter list –based on the parent object  level. \n",
      "• Click on the Load Import – request will be submitted,  and result ID should be generated. \n",
      " \n",
      " - User Manual  | Confidential  Page | 16 \n",
      "➢ Recon cile – When we click this button, it hits the template and compares.  \n",
      "➢ Reconcile Report – Generates all the reports Pass/Fail . \n",
      "➢ Rejection Report  – Only the details of rejected results in reconcile.  \n",
      "Reconciliation  \n",
      "Data reconciliation (DR) is a term typically used to describe  a verific ation phase during a data migration \n",
      "where the target data is compared against original source data to ensure that the migration architecture has \n",
      "transferred the data correctly.  Reconciliation is the process of ensuring that two sets of records are in \n",
      "agree ment.   \n",
      "• Click on the hamburger button - Dash Boards – Reconcile – select object,  cloud template,  cloud  batch - \n",
      "view. \n",
      "• We will get the results of source data extracted, validated and data migrated to the cloud . \n",
      "• We can check error records and details.  \n",
      "Other  Screens  \n",
      "Load Cockpit:  \n",
      "Let’s say object and the flow is already created – to load the data - transform and cloud data loading for the \n",
      "same parent object with different child objects in the same screen.  \n",
      "• We can use Load Cockpit option from the Hamburger button - Automation - Load Cockpit – New – fill up all \n",
      "the information.  \n",
      "• Cloud and Source templates should be created/available to use Load Cockpit option. \n",
      "• Click on Load Source Data button – same functionality as to Load Data from Source Template \n",
      "Workbench. \n",
      " \n",
      " - User Manual  | Confidential  Page | 17 \n",
      "• Enter the U nique Batch Name.  \n",
      "• Click on the T ransform to validate all mapping of Cloud Template Workbench screen. \n",
      "• If you want to upload file manually, choose manual radio button . \n",
      "• If the  file-based data is different for all the object s– then upload the file from Load Source ( Denormalize ) \n",
      "and if file based data is same for all the objects then File can be uploaded from load source (Normalise) . \n",
      "Source Conversions workbench:  \n",
      "• Click on the Hamburger button - ConvertRite – Source Conversions Workbench - select the template to \n",
      "check  failed records. \n",
      "• In this screen we can check  and access  failed records when we are extracting/loading source data . \n",
      "• Download option under status will be enabled only when we use external option to upload file in the \n",
      "source template workb ench .\n",
      " - User Manual  | Confidential  Page | 18 \n",
      "HCM Load Import:  \n",
      "• Click on the hamburger button  - ConvertRite – HCM Load import – New –fill all required details  - Load \n",
      "and Import. \n",
      "• HCM Load Import screen is used to migrate data directly into cloud when we are dealing with HCM object \n",
      "codes.  \n",
      "• Summary Button hits the summary of the cloud table and Summary report generates the file with \n",
      "summary data.  \n",
      "Date Configuration:  \n",
      "• Click on the hamburger button – Configuration – Date Configuration  - New- Fill details - Save.\n",
      "• Date configuration screen is used to change the source date format before converting the data as the \n",
      "required date format in cloud.  \n",
      "FBDI Workbench (XLSM):  \n",
      "• Click on the hamburger button – Sequence Generator/Grouping – FBDI Workbench (X lsm) -New – Save. \n",
      "• FBDI Workbench ( Xlsm) screen is used to generate sequence based on the xlsm file in  the cloud \n",
      "database.  \n",
      "➢ Download comments – downloads the column details with description.  \n",
      "➢ Save as Blob – History of object sequence with version will be saved for future reference. \n",
      " \n",
      " - User Manual  | Confidential  Page | 19 \n",
      "Object Grouping:  \n",
      "• Click on the hamburger button - Sequence Generator/Grouping – Object Grouping . \n",
      "• Object grouping is used to group the objects based on Excel File of Cloud. \n",
      "• If we do Objec t grouping, template grouping should be done.  \n",
      "Template Grouping:  \n",
      "• Click on  the hamburger button - Sequence Generator/Grouping –Template Grouping .\n",
      "• Template Grouping screen is used to group the same objects that are grouped in  the object grouping by \n",
      "selecting template.  \n",
      "Cloud Sql Adhoc Query:  \n",
      "• Click on the hamburger button – Master data – Cloud SQL Adhoc Query  –New – Write SQL query . \n",
      "• This screen is used to get the lookups from cloud with SQL query.  \n",
      "• Check the Update cloud lookup data box to update data in the lookup.  \n",
      "Load Cockpit – HCM:  \n",
      "• Click on the hamburger button - Automation  – Load Cockpit HCM  – fill the required info – Load Source \n",
      "Data (Auto) – Transform – HCM Cloud Import .\n",
      " \n",
      " - User Manual  | Confidential  Page | 20\n",
      "• This screen is used to do the process in on e screen for HCM object codes.  \n",
      "Status of Jobs:  \n",
      "• Click on  the hamburger button -Dash Boards – Status of jobs .\n",
      " Help:  \n",
      "• Click on  the hamburger button – Dash Boards – Help. \n",
      ".  \n",
      " \n",
      " \n",
      " - User Manual  | Confidential  Page | 21 \n",
      "Abbreviations  \n",
      "Abbreviation  Full Form  \n",
      "ERP Enterprise Resource Planning  \n",
      "POD  Product Oriented Delivery  \n",
      "HDL HCM Data Loader  \n",
      "FBDI  File Based Data Import  \n",
      "Orig Trans Ref Original Transaction References  \n",
      "BU Business Unit \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' \\nUser Manual ',\n",
       " 'Table of Contents  \\nIntroduction ................................ ................................ ................................ ................................ ..............  3 \\nHigh Level Process Flow  ................................ ................................ ................................ .........................  3 \\nLogin Information  ................................ ................................ ................................ ................................ .... 3 \\nWorkflow ................................ ................................ ................................ ................................ ..................  4 \\nReconciliation  ................................ ................................ ................................ ................................ ........  16 \\nOther  Screens  ................................ ................................ ................................ ................................ ....... 16 \\nAbbreviations  ................................ ................................ ................................ ................................ ........  21 \\n ',\n",
       " 'Introduction  \\nIt is a data conversion tool built on Oracle technologies, designed to automate data conversion from any \\nsource to Oracle Cloud Applications. With ConvertRite, you can automate manual, time -consuming and \\nerror -prone processes (such as data mapping and validation) and convert all your data from legacy \\napplications to make it compatible with Oracle Cloud Applications.  \\nHigh Level Process Flow',\n",
       " 'Login Information  \\nLink: https://convertrite.ritesoftware.com/  \\nLogin: The user should log in and select the role of admin to access ConvertRite.   ',\n",
       " 'Workflow  \\nClick on hamburger menu  on top left \\nfor application navigation.  \\n      ',\n",
       " ' \\n➢ Project, POD, Parent object and child object  (object code)  are \\nMaster data.  \\nSTEP 1:  \\nCreation  of POD:  \\nPOD - The Product Oriented Delivery (POD) model is  a software development strategy that centers on \\nbuilding small cross -functional teams that own specific tasks or requirements for a project.  \\nA project can be created in different PODs or many Projects in one POD can be \\ncreated.  \\n• To create  a POD in ConvertRite, click on hamburger button - master data - \\nlookup workbench - search for POD – click on add at lookup values  – add POD \\nname.  (POD  is independent - doesn’t rel y on any other values) – save.',\n",
       " '➢ Export CSV – Export CSV button downloads all the file based data on this \\nparticular screen. ',\n",
       " 'Creation of Project : \\n• To create the project, in the lookup header – search for  the Lookup name as Project and add the Lookup \\nvalue – your project details. ',\n",
       " '• Project is related to POD – Hence select the required POD  – Save .',\n",
       " 'Creation of Parent Object:  \\n• Click on the hamburger button - Master data - Lookup workbench .  \\n• Click on Search  -Search for Parent object - add a lookup value - define lookup name as required parent \\nobject from FBDI file ( Create parent obj ect) – Assign project name in actual value .',\n",
       " '• If the project is not listed under actual value, need to add project details under project name lookup name. ',\n",
       " 'Creation of Child Object:  \\n• Search for lookup name – object  code  – add the lookup value for child object from FBDI file  which should \\nbe related to  the parent object code. ',\n",
       " ' \\nStep 2: \\nAssign POD  and Project : \\n• Click on the Hamburger button –ConvertRite  – Manage your project – New. \\n• Define project name, POD and all mandatory information - save  (POD environment will be assigned to  the \\nproject and object in this screen) . \\n         \\n➢ Copy project is used to copy the complete project with same data into a different POD with project \\nstatus, start and completion date. ',\n",
       " '',\n",
       " '• Click on Load WBS – Project will be assigned to parent object. ',\n",
       " '• If the project is created already, search for the required project and Load WBS [it can be done for the \\nother parent object (Load WBS is parent object level) ] \\nStep 3:  \\nAssign Role Object:  \\n• We must  Enable flag  to activate the  load metadata option.   \\n• Click on the Admin – switch to SuperUser.',\n",
       " ' \\n• In the SuperUser profile, click on the hamburger button on top left - admin – role object. ',\n",
       " '',\n",
       " '• Select the role name as admin, required POD, Project,  parent object – Enable flag. Once done, switch \\nback to admin. ',\n",
       " '✓ Master data configuration done . \\n• We must configure and connect cloud/source to database to get metadata so that stagging table can be \\ncreated.  \\n• On cloud side after completing configuration – load metadata should be done, on source  side – both can \\nbe done on same screen.  \\nStep 4:  \\nSource  Configuration:  (Parent object level)  \\n• Click on the Hamburger button - Configuration – External Source Configuration – New – fill up info rmation  \\n– Save ( Parent object level) – Upload file (View file provided by functional team). ',\n",
       " '• If parent object is available, we can use update view option to update any details . (To update view file for \\neach individual child object) . \\n• Click on the load metadata – structure is done .',\n",
       " 'Step 5:  \\nSource Template workbench:  \\n• Click on the Hamburger button -ConvertRite - Source Template Workbench - New – fill all required fields – \\nSave- Create Table – Import Columns.     ',\n",
       " '➢ BU Specific must  be enabled assign the template at business user level.  \\n➢ Check the Normalize box to differentiate duplicate data (Process will be stopped if any duplicate data \\nis identified) . \\n• Select all columns displayed - Save. \\n• Click on the Baseline – Stagging table name and view name will be displayed if we click on three dots \\n(Stagging table will have all the data but view have only required data) . \\n• Select Ori g trans ref - To link source and cloud columns data , unique identification for the records  and \\nalso when  we n eed 2 or more columns data to merge the data to 1 column in cloud . \\n➢ Re Orig  Tras ref is used when an Orig trans ref is already created and requirement is \\nchanged/updated - we can update re Orig trans based on the batch name . \\n• Load Data – success (make a note  of the batch name – unique name should be given by us) .',\n",
       " '➢ While loading data, choose manual  to upload small data through file and choose external option to \\nupload large data. ',\n",
       " '',\n",
       " 'Step 6: \\nCloud Access Configuration:  \\n• Click on  the Hamburger button – Configuration  – Cloud Access configuration.  \\n• Cloud Access Configuration screen is used to connect to  the cloud SaaS environment with the required \\ncredentials.  \\n• Cloud URL – SaaS URL and credentials will be provided by the functional team. ',\n",
       " 'Step 7:  \\nCloud Configuration:  \\n• Click on the Hamburger button – Configuration – Cloud Configuration . \\n• Click on New –Define all re quired  info (From FBDI ): Object Code – Ctrl File Name – Xlsm F ile Name -\\nSheet Name- Interface Table.',\n",
       " '• Hence Connected to cloud. We must get the structure – through load meta data step. ( Only after  role \\nobject -parent object level) .',\n",
       " 'Step 8:  \\nCloud Load Metadata:  \\n• Click on the Hamburger button -Master Data- Load metadata - select Cloud – Cloud Load Metadata .',\n",
       " '• On the pop - up screen, select all the required data  – Metadata table will be created. ',\n",
       " '➢ We can load metadata to source by selecting Source radio - click on EBS adapter.  \\n➢ By Clicking on EBS adapter, it will re -direct to external source co nfiguration screen  where we can \\nselect particular parent object to create metadata , click on upload file and load data.  \\nStep 9: \\nCloud Stagging table:  \\n• Click on  the Hamburger button  - ConvertRite  – Cloud Template Workbench – New – fill all required \\ninformation - Save – Create Table – Import Columns. ',\n",
       " '• In the source template field- the new template we have  created should be available,  select that source \\ntemplate  and SAVE . ',\n",
       " '',\n",
       " ' \\n• Source columns will be added and listed on this screen . \\n• On the Cloud Template Workbench – click on the anchor icon ( on top right) which is called user hooks \\n(Extraction, validation & transformation, and cloud import) .',\n",
       " ' \\n• We can enable pre or post hook to manipulate any data before or afte r validation . \\n• Hook value = Any SQL query ( As provided) . \\nStep 10:  \\nSequence Generator:  \\n• Now to generate sequence – go to hamburger button - Sequence Generator/ Grouping – FBDI Workbench \\n(Ctrl) – New - Fill all the details  – choose Auto  – Save. \\n• Note: We can generate sequence using Ctrl file [FBDI workbench (ctrl)]   or  Xlsm file [FBDI workbench \\n(xlsm)]',\n",
       " '• Switch back to Cloud Template Workbench – Seq+ ctrl button will be enabled, click on it and sort \\nsequence .',\n",
       " '',\n",
       " 'Step 11:   \\n• We must do the mapping (after generating sequence) – information will be provided by source/functional \\nteam.  \\nMapping types:   \\n1. As – Is - Moves X - X \\n2. Prefix  - Add X - …... (Before data value)  \\n3. Suffix  - Add …... - X (After the data value)  \\n4. Constant  - Source no v alue but should sent constant value to cloud  \\n5. One to One  - One field of source will be mapped to one cloud value  \\n6. Two to One  - Two fields of source will be mapped to one cloud value  \\n7. Three to One  - Three fields of source will be mapped to one cloud value  \\n8. Formula set - Write query to do other conversions (optional values)  \\n• Click on the hamburger button - Data Transformation - Define Mapping Set (To Define  1-1, 2-1, 3-1) -New- \\ndefine all information and save.  \\n• Once the mapping set is saved, add mapping set value s (column – column) .',\n",
       " '• Source Object  - View file  from the source template  \\n• Source Column – Column name in the source side  \\n• Source Field – Column value in the source side  \\n• Cloud Column – Column name from the cloud s ide (This  information will be provided by functional  team ) \\n• Cloud Value –Column  value to be updated on the cloud side \\n• We must  write the condition for 1 -1, 2-1 and 3 -1 mappings in W here Clause .',\n",
       " '• SQL query will be generated by the application  on the backend  and hence transforms the data . \\nDefine Mapping Set: \\n• To define any other mapping type to transform the data  - Click on menu - Data Transformation - Define \\nFormula Set –we can write required SQL query.  \\n• We have 3 Formula types – Java, SQL and function . \\n• Test SQL is used to test the SQL query written by us.  \\n• Test Data  is used to test the data with given SQL query at Orig trans level.  \\n• Original Trans Ref – The records that we select as org trans ref in  the source side . \\n• Formula value – Expected value should be given. \\n• SQL Query – Should be written on Org trans ref . \\n• Test SQL – To test the SQL query written . \\n• Test Data – To test the data that we got by the sql query written. ',\n",
       " 'Step 1 2: \\nIn Cloud template workbench:  \\n• Select all the columns for which sequence is generated – Save - Click on Baseline to create table. ',\n",
       " 'Step 1 3:      \\nValidation : \\n• Click on the Hamburger  button - ConvertRite - Cloud Conversions Workbench – select  the template name \\nand batch name [get that from source template workbench(unique) ]',\n",
       " '',\n",
       " '• Click on the transform to validate and convert.  \\n• Once done , ConvertRite  application needs to be connected to oracle fusion  to run the jobs.   \\n➢ Re-Process option is used to re -validate only failed records after correcting the errors.  \\n➢ FBDI – File based data import - is the file generated after the conver sions in our application before \\nmoving into the cloud.  \\n➢ FBDI – is only for ERP modelling, HDL – is for HCM related data. ',\n",
       " 'Step 1 4:    \\nLoad Import Metadata:  \\n• So, click on the hamburger button -Go to the Master  Data option under  – Load Import Metadata – fill the \\ninformation provided by  the respective  functional team – Save.',\n",
       " 'Step 1 5:      \\nCloud Load Import:  \\n• Click on the hamburger button - ConvertRite - Cloud Load Import – select  template name , batch name, \\nparameter list –based on the parent object  level. ',\n",
       " '• Click on the Load Import – request will be submitted,  and result ID should be generated. ',\n",
       " '➢ Recon cile – When we click this button, it hits the template and compares.  \\n➢ Reconcile Report – Generates all the reports Pass/Fail . \\n➢ Rejection Report  – Only the details of rejected results in reconcile.  \\nReconciliation  \\nData reconciliation (DR) is a term typically used to describe  a verific ation phase during a data migration \\nwhere the target data is compared against original source data to ensure that the migration architecture has \\ntransferred the data correctly.  Reconciliation is the process of ensuring that two sets of records are in \\nagree ment.   \\n• Click on the hamburger button - Dash Boards – Reconcile – select object,  cloud template,  cloud  batch - \\nview. ',\n",
       " '• We will get the results of source data extracted, validated and data migrated to the cloud . \\n• We can check error records and details.  \\nOther  Screens  \\nLoad Cockpit:  \\nLet’s say object and the flow is already created – to load the data - transform and cloud data loading for the \\nsame parent object with different child objects in the same screen.  \\n• We can use Load Cockpit option from the Hamburger button - Automation - Load Cockpit – New – fill up all \\nthe information.  \\n• Cloud and Source templates should be created/available to use Load Cockpit option. ',\n",
       " '• Click on Load Source Data button – same functionality as to Load Data from Source Template \\nWorkbench. ',\n",
       " '• Enter the U nique Batch Name.  \\n• Click on the T ransform to validate all mapping of Cloud Template Workbench screen. ',\n",
       " '• If you want to upload file manually, choose manual radio button . \\n• If the  file-based data is different for all the object s– then upload the file from Load Source ( Denormalize ) \\nand if file based data is same for all the objects then File can be uploaded from load source (Normalise) . \\nSource Conversions workbench:  \\n• Click on the Hamburger button - ConvertRite – Source Conversions Workbench - select the template to \\ncheck  failed records. ',\n",
       " '• In this screen we can check  and access  failed records when we are extracting/loading source data . \\n• Download option under status will be enabled only when we use external option to upload file in the \\nsource template workb ench .',\n",
       " 'HCM Load Import:  \\n• Click on the hamburger button  - ConvertRite – HCM Load import – New –fill all required details  - Load \\nand Import. ',\n",
       " '• HCM Load Import screen is used to migrate data directly into cloud when we are dealing with HCM object \\ncodes.  \\n• Summary Button hits the summary of the cloud table and Summary report generates the file with \\nsummary data.  \\nDate Configuration:  \\n• Click on the hamburger button – Configuration – Date Configuration  - New- Fill details - Save.',\n",
       " '• Date configuration screen is used to change the source date format before converting the data as the \\nrequired date format in cloud.  \\nFBDI Workbench (XLSM):  \\n• Click on the hamburger button – Sequence Generator/Grouping – FBDI Workbench (X lsm) -New – Save. ',\n",
       " '• FBDI Workbench ( Xlsm) screen is used to generate sequence based on the xlsm file in  the cloud \\ndatabase.  \\n➢ Download comments – downloads the column details with description.  \\n➢ Save as Blob – History of object sequence with version will be saved for future reference. ',\n",
       " 'Object Grouping:  \\n• Click on the hamburger button - Sequence Generator/Grouping – Object Grouping . \\n• Object grouping is used to group the objects based on Excel File of Cloud. ',\n",
       " '• If we do Objec t grouping, template grouping should be done.  \\nTemplate Grouping:  \\n• Click on  the hamburger button - Sequence Generator/Grouping –Template Grouping .',\n",
       " '• Template Grouping screen is used to group the same objects that are grouped in  the object grouping by \\nselecting template.  \\nCloud Sql Adhoc Query:  \\n• Click on the hamburger button – Master data – Cloud SQL Adhoc Query  –New – Write SQL query . ',\n",
       " '• This screen is used to get the lookups from cloud with SQL query.  \\n• Check the Update cloud lookup data box to update data in the lookup.  \\nLoad Cockpit – HCM:  \\n• Click on the hamburger button - Automation  – Load Cockpit HCM  – fill the required info – Load Source \\nData (Auto) – Transform – HCM Cloud Import .',\n",
       " '',\n",
       " '• This screen is used to do the process in on e screen for HCM object codes.  \\nStatus of Jobs:  \\n• Click on  the hamburger button -Dash Boards – Status of jobs .',\n",
       " ' Help:  \\n• Click on  the hamburger button – Dash Boards – Help. \\n.  \\n ',\n",
       " 'Abbreviations  \\nAbbreviation  Full Form  \\nERP Enterprise Resource Planning  \\nPOD  Product Oriented Delivery  \\nHDL HCM Data Loader  \\nFBDI  File Based Data Import  \\nOrig Trans Ref Original Transaction References  \\nBU Business Unit ']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs = [string for string in paragraphs if string.strip()]\n",
    "new_paragraphs=[]\n",
    "for string in paragraphs:\n",
    "    print(string)\n",
    "    newstring = re.sub(r\"^\\s*-\\s*User Manual\\s*\\|\\s*Confidential\\s*Page\\s*\\|\\s*\\d+\\s*\", \"\", string)\n",
    "    new_paragraphs.append(newstring)\n",
    "new_paragraphs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nUser Manual</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Table of Contents  \\nIntroduction ...............</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction  \\nIt is a data conversion tool b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Login Information  \\nLink: https://convertrite...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Workflow  \\nClick on hamburger menu  on top le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>• This screen is used to get the lookups from ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>• This screen is used to do the process in on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Help:  \\n• Click on  the hamburger button – D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Abbreviations  \\nAbbreviation  Full Form  \\nER...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 para\n",
       "0                                      \\nUser Manual \n",
       "1   Table of Contents  \\nIntroduction ...............\n",
       "2   Introduction  \\nIt is a data conversion tool b...\n",
       "3   Login Information  \\nLink: https://convertrite...\n",
       "4   Workflow  \\nClick on hamburger menu  on top le...\n",
       "..                                                ...\n",
       "60  • This screen is used to get the lookups from ...\n",
       "61                                                   \n",
       "62  • This screen is used to do the process in on ...\n",
       "63   Help:  \\n• Click on  the hamburger button – D...\n",
       "64  Abbreviations  \\nAbbreviation  Full Form  \\nER...\n",
       "\n",
       "[65 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_paragraphs\n",
    "paragraphs = pd.DataFrame(new_paragraphs, columns=['para'])\n",
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nUser Manual Table of Contents  \\nIntroducti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creation of Project  \\n To create the project ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Click on Load WBS  Project will be assigned t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Step 5  \\nSource Template workbench  \\n Click ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Step 8  \\nCloud Load Metadata  \\n Click on the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Switch back to Cloud Template Workbench  Seq ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Click on the transform to validate and conver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Click on Load Source Data button  same functi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FBDI Workbench  Xlsm screen is used to genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Help  \\n Click on  the hamburger button  Dash...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk\n",
       "0   \\nUser Manual Table of Contents  \\nIntroducti...\n",
       "1  Creation of Project  \\n To create the project ...\n",
       "2   Click on Load WBS  Project will be assigned t...\n",
       "3  Step 5  \\nSource Template workbench  \\n Click ...\n",
       "4  Step 8  \\nCloud Load Metadata  \\n Click on the...\n",
       "5   Switch back to Cloud Template Workbench  Seq ...\n",
       "6   Click on the transform to validate and conver...\n",
       "7   Click on Load Source Data button  same functi...\n",
       "8   FBDI Workbench  Xlsm screen is used to genera...\n",
       "9   Help  \\n Click on  the hamburger button  Dash..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = pd.DataFrame(chunks_final, columns=['chunk'])\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Switch back to Cloud Template Workbench  Seq ctrl button will be enabled click on it and sort \\nsequence '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks_final[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                        \\nUser Manual \n",
       "1     Table of Contents  \\nIntroduction ...............\n",
       "2     Introduction  \\nIt is a data conversion tool b...\n",
       "3     Login Information  \\nLink: https://convertrite...\n",
       "4     Workflow  \\nClick on hamburger menu  on top le...\n",
       "5      \\n➢ Project, POD, Parent object and child obj...\n",
       "6     ➢ Export CSV – Export CSV button downloads all...\n",
       "7     Creation of Project : \\n• To create the projec...\n",
       "8     • Project is related to POD – Hence select the...\n",
       "9     Creation of Parent Object:  \\n• Click on the h...\n",
       "10    • If the project is not listed under actual va...\n",
       "11    Creation of Child Object:  \\n• Search for look...\n",
       "12     \\nStep 2: \\nAssign POD  and Project : \\n• Cli...\n",
       "13                                                     \n",
       "14    • Click on Load WBS – Project will be assigned...\n",
       "15    • If the project is created already, search fo...\n",
       "16     \\n• In the SuperUser profile, click on the ha...\n",
       "17                                                     \n",
       "18    • Select the role name as admin, required POD,...\n",
       "19    ✓ Master data configuration done . \\n• We must...\n",
       "Name: para, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs['para'][:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nUser Manual Table of Contents  \\nIntroducti...</td>\n",
       "      <td>[0.008730674162507057, 0.008423255756497383, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Creation of Project  \\n To create the project ...</td>\n",
       "      <td>[0.0012297879438847303, -0.005864111240953207,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Click on Load WBS  Project will be assigned t...</td>\n",
       "      <td>[0.010615729726850986, -0.01672079600393772, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Step 5  \\nSource Template workbench  \\n Click ...</td>\n",
       "      <td>[-0.009413506835699081, -0.001670914702117443,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Step 8  \\nCloud Load Metadata  \\n Click on the...</td>\n",
       "      <td>[-0.00808194000273943, -0.00602630153298378, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Switch back to Cloud Template Workbench  Seq ...</td>\n",
       "      <td>[-0.021586423739790916, -0.016960762441158295,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Click on the transform to validate and conver...</td>\n",
       "      <td>[-0.017562050372362137, 0.014654378406703472, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Click on Load Source Data button  same functi...</td>\n",
       "      <td>[-0.013803873211145401, -0.014669110998511314,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FBDI Workbench  Xlsm screen is used to genera...</td>\n",
       "      <td>[-0.03789273649454117, -0.00738043338060379, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Help  \\n Click on  the hamburger button  Dash...</td>\n",
       "      <td>[-0.010759570635855198, 0.003916266839951277, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk  \\\n",
       "0   \\nUser Manual Table of Contents  \\nIntroducti...   \n",
       "1  Creation of Project  \\n To create the project ...   \n",
       "2   Click on Load WBS  Project will be assigned t...   \n",
       "3  Step 5  \\nSource Template workbench  \\n Click ...   \n",
       "4  Step 8  \\nCloud Load Metadata  \\n Click on the...   \n",
       "5   Switch back to Cloud Template Workbench  Seq ...   \n",
       "6   Click on the transform to validate and conver...   \n",
       "7   Click on Load Source Data button  same functi...   \n",
       "8   FBDI Workbench  Xlsm screen is used to genera...   \n",
       "9   Help  \\n Click on  the hamburger button  Dash...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.008730674162507057, 0.008423255756497383, -...  \n",
       "1  [0.0012297879438847303, -0.005864111240953207,...  \n",
       "2  [0.010615729726850986, -0.01672079600393772, -...  \n",
       "3  [-0.009413506835699081, -0.001670914702117443,...  \n",
       "4  [-0.00808194000273943, -0.00602630153298378, -...  \n",
       "5  [-0.021586423739790916, -0.016960762441158295,...  \n",
       "6  [-0.017562050372362137, 0.014654378406703472, ...  \n",
       "7  [-0.013803873211145401, -0.014669110998511314,...  \n",
       "8  [-0.03789273649454117, -0.00738043338060379, -...  \n",
       "9  [-0.010759570635855198, 0.003916266839951277, ...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks['embedding'] = chunks['chunk'].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>para</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nUser Manual</td>\n",
       "      <td>[-0.013203667476773262, 0.002843025606125593, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Table of Contents  \\nIntroduction ...............</td>\n",
       "      <td>[0.01696472056210041, 0.006556727457791567, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Introduction  \\nIt is a data conversion tool b...</td>\n",
       "      <td>[-0.014939437620341778, -0.016217974945902824,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Login Information  \\nLink: https://convertrite...</td>\n",
       "      <td>[0.0026779407635331154, -0.0032268769573420286...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Workflow  \\nClick on hamburger menu  on top le...</td>\n",
       "      <td>[0.006495112553238869, 0.002556370571255684, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n➢ Project, POD, Parent object and child obj...</td>\n",
       "      <td>[0.019584445282816887, -0.003975201863795519, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>➢ Export CSV – Export CSV button downloads all...</td>\n",
       "      <td>[-0.028524329885840416, -0.024643193930387497,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Creation of Project : \\n• To create the projec...</td>\n",
       "      <td>[-0.005877157673239708, 0.010511070489883423, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>• Project is related to POD – Hence select the...</td>\n",
       "      <td>[0.00712668476626277, -0.023890431970357895, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Creation of Parent Object:  \\n• Click on the h...</td>\n",
       "      <td>[0.007570208515971899, 0.013928638771176338, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                para  \\\n",
       "0                                     \\nUser Manual    \n",
       "1  Table of Contents  \\nIntroduction ...............   \n",
       "2  Introduction  \\nIt is a data conversion tool b...   \n",
       "3  Login Information  \\nLink: https://convertrite...   \n",
       "4  Workflow  \\nClick on hamburger menu  on top le...   \n",
       "5   \\n➢ Project, POD, Parent object and child obj...   \n",
       "6  ➢ Export CSV – Export CSV button downloads all...   \n",
       "7  Creation of Project : \\n• To create the projec...   \n",
       "8  • Project is related to POD – Hence select the...   \n",
       "9  Creation of Parent Object:  \\n• Click on the h...   \n",
       "\n",
       "                                           embedding  \n",
       "0  [-0.013203667476773262, 0.002843025606125593, ...  \n",
       "1  [0.01696472056210041, 0.006556727457791567, -0...  \n",
       "2  [-0.014939437620341778, -0.016217974945902824,...  \n",
       "3  [0.0026779407635331154, -0.0032268769573420286...  \n",
       "4  [0.006495112553238869, 0.002556370571255684, -...  \n",
       "5  [0.019584445282816887, -0.003975201863795519, ...  \n",
       "6  [-0.028524329885840416, -0.024643193930387497,...  \n",
       "7  [-0.005877157673239708, 0.010511070489883423, ...  \n",
       "8  [0.00712668476626277, -0.023890431970357895, 0...  \n",
       "9  [0.007570208515971899, 0.013928638771176338, -...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs['embedding'] = paragraphs['para'][:10].apply(lambda x: get_embedding(x, engine='text-embedding-ada-002'))\n",
    "paragraphs.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import cosine_similarity\n",
    "\n",
    "search_query = \"create pod\"\n",
    "search_query_emd = get_embedding(search_query,engine= \"text-embedding-ada-002\")\n",
    "# paragraphs[\"similarities\"] = paragraphs['embedding'].apply(lambda x: cosine_similarity(x, search_query_emd))\n",
    "chunks[\"similarities\"] = chunks['embedding'].apply(lambda x: cosine_similarity(x, search_query_emd))\n",
    "\n",
    "# chunkstemp = chunks.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "newchunks = chunks.sort_values('similarities',ascending=False)\n",
    "ans = newchunks.iloc[0]['chunk']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of Project  \n",
      " To create the project in the lookup header  search for  the Lookup name as Project and add the Lookup \n",
      "value  your project details  Project is related to POD  Hence select the required POD   Save \n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tts = pyttsx3.init()\n",
    "\n",
    "# Set the TTS voice and tone\n",
    "voice_id = \"english+f3\"\n",
    "tts.setProperty(\"voice\", voice_id)\n",
    "tts.setProperty(\"rate\", 120)  # Increase the speaking rate for a more energetic tone\n",
    "\n",
    "# Speak the summary\n",
    "tts.say(ans)\n",
    "tts.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creation of Project  \n",
      " To create the project in the lookup header  search for  the Lookup name as Project and add the Lookup \n",
      "value  your project details  Project is related to POD  Hence select the required POD   Save \n",
      "\n",
      "to the current project and copy the new entry into the entry string \n",
      "\n",
      "to this name, which contains Project. the default name\n",
      "\n",
      "and it is the name of the POD, and you are now\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Generate a paraphrased version of the summary\n",
    "input_ids = tokenizer.encode(ans, return_tensors='pt')\n",
    "pad_token = '<pad>'\n",
    "tokenizer.add_tokens([pad_token])\n",
    "\n",
    "# Set the padding token as the padding token ID\n",
    "tokenizer.pad_token = pad_token\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(pad_token)\n",
    "if input_ids is not None:\n",
    "    attention_mask = input_ids.ne(tokenizer.pad_token_id)\n",
    "    generated = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_length=100,\n",
    "        do_sample=True,\n",
    "        top_p=0.92,\n",
    "        top_k=50\n",
    "    )\n",
    "    paraphrase = tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "    print(paraphrase)\n",
    "else:\n",
    "    print(\"Input is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Tokenizer(name_or_path='gpt2', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True)})\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb79ba29542c144dd811710a33c3f672d46a730a07e871a5c60f51fcfe992ed4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
